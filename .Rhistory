wikiCompanyList <- fread(file = "wikiPage_company_zip.csv", data.table = F, encoding = "UTF-8")
wikiCompanyList <- wikiCompanyList %>%
as_data_frame() %>%
rename(companyName = COMNAM,
wikiPageName = WIKI) %>%
select(companyName, wikiPageName) %>%
na.omit()
# 設定資料下載路徑(可自行設定)
filePath <- "D:/MiaoLing Project/wiki"
#filePath <- getwd()
# 設定資料下載年月範圍
downloadStartDate <- "2014/02/01"
downloadEndDate <- "2014/12/31"
targetYearMonth <- seq(as.Date(downloadStartDate), as.Date(downloadEndDate), "months")
targetYearMonth <- tibble(year = substring(targetYearMonth, first = 1, last = 4),
yearMonth = substring(targetYearMonth, first = 1, last = 7))
# 迴圈日期清單
ix <- 1
for(ix in c(2:nrow(targetYearMonth))){
# 目標頁面網址
url <- paste0("https://dumps.wikimedia.org/other/pagecounts-raw/",
targetYearMonth$year[ix],"/", targetYearMonth$yearMonth[ix],"/")
dir.create(targetYearMonth$yearMonth[ix])
# 建立儲存表
# storeTable <- NULL
# 取得頁面連結資訊
downloadFileName <- read_html(url) %>%
html_nodes(xpath = "//li/a") %>%
html_text()
# 只下載pagecounts頁面(在2010年5月時會有pagecounts和projectcount連結)
downloadFileName <- downloadFileName[grepl("pagecounts", downloadFileName)]
# 迴圈頁面連結資訊以下載資料
filePath <-paste0("D:/MiaoLing Project/wiki",'/',targetYearMonth$yearMonth[ix])
iy <- 1
for(iy in c(1:length(downloadFileName))){
cat(paste0("目前正在下載年月: ", targetYearMonth$yearMonth[ix]), "進度: ", iy, "/", length(downloadFileName),"\n")
# 防呆機制:(20181210:發現部分檔案有毀損狀況導致無法順利解壓縮 故做此處理)
if(downloadFileName[iy] %in% c("pagecounts-20130618-150000.gz")){
next
}
# 資料名稱
targetFilePathName <- paste0(filePath, "/", downloadFileName[iy])
# 切割名稱方便儲存資訊
dataDate <- strsplit(downloadFileName[iy], split = "-")[[1]][2] %>% as.numeric()
dataTime <- strsplit(downloadFileName[iy], split = "-")[[1]][3]
dataTime <- strsplit(dataTime, split = "\\.")[[1]][1] %>% as.numeric()
dataTime <- floor(dataTime/10000)
# 下載資料
download.file(url = paste0(url, downloadFileName[iy]), destfile = targetFilePathName)
# 解壓縮
#gunzip(filename = targetFilePathName, overwrite = T)
## 判斷是否為空資料 若為空資料則不做併表 直接刪除資料進入下一階段
# if(file.size(gsub(".gz", "", targetFilePathName)) != 0){
#
#
#   # 讀取資料
#   # 此處略過前5行不讀取 因為前5行有時候是亂碼 R看不懂
#   # 略過5行沒關係 因為重點是要抓language=en的資料 language是按字母順序排的
#   data <- read.table(file = gsub(".gz", "", targetFilePathName), quote="", sep = " " ,
#                      header = F, stringsAsFactors= F, fill = TRUE, skip = 2000) %>%
#     as_data_frame() %>%
#     rename(language = V1, pageName = V2, viewNums = V3)
#
#   # 判斷是否有要找的維基關鍵頁面
#   keyPage <- data %>%
#     filter(language == "en") %>%
#     filter(pageName %in% wikiCompanyList$wikiPageName) %>%
#     group_by(pageName) %>%
#     summarise(viewNums = sum(as.numeric(viewNums), na.rm = T)) %>%
#     transmute(dataDate, dataTime, pageName, viewNums)
#
#   # 儲存資料
#   storeTable <- storeTable %>% bind_rows(keyPage)
# }
#
# # 刪除資料
# file.remove(gsub(".gz", "", targetFilePathName))
}
# 由每小時頻率轉為每日資料
# storeTable <- storeTable %>%
#   group_by(dataDate, pageName) %>%
#   summarise(viewNums = sum(viewNums, na.rm = T))
#
# # 本月份資料跑完 儲存資料
# write.csv(storeTable, file = paste0(targetYearMonth$yearMonth[ix], "-wiki-page-view-numbers.csv"), row.names = F)
}
cat("程式已執行完畢!")
for(iy in c(296:length(downloadFileName))){
cat(paste0("目前正在下載年月: ", targetYearMonth$yearMonth[ix]), "進度: ", iy, "/", length(downloadFileName),"\n")
# 防呆機制:(20181210:發現部分檔案有毀損狀況導致無法順利解壓縮 故做此處理)
if(downloadFileName[iy] %in% c("pagecounts-20130618-150000.gz")){
next
}
# 資料名稱
targetFilePathName <- paste0(filePath, "/", downloadFileName[iy])
# 切割名稱方便儲存資訊
dataDate <- strsplit(downloadFileName[iy], split = "-")[[1]][2] %>% as.numeric()
dataTime <- strsplit(downloadFileName[iy], split = "-")[[1]][3]
dataTime <- strsplit(dataTime, split = "\\.")[[1]][1] %>% as.numeric()
dataTime <- floor(dataTime/10000)
# 下載資料
download.file(url = paste0(url, downloadFileName[iy]), destfile = targetFilePathName)
# 解壓縮
#gunzip(filename = targetFilePathName, overwrite = T)
## 判斷是否為空資料 若為空資料則不做併表 直接刪除資料進入下一階段
# if(file.size(gsub(".gz", "", targetFilePathName)) != 0){
#
#
#   # 讀取資料
#   # 此處略過前5行不讀取 因為前5行有時候是亂碼 R看不懂
#   # 略過5行沒關係 因為重點是要抓language=en的資料 language是按字母順序排的
#   data <- read.table(file = gsub(".gz", "", targetFilePathName), quote="", sep = " " ,
#                      header = F, stringsAsFactors= F, fill = TRUE, skip = 2000) %>%
#     as_data_frame() %>%
#     rename(language = V1, pageName = V2, viewNums = V3)
#
#   # 判斷是否有要找的維基關鍵頁面
#   keyPage <- data %>%
#     filter(language == "en") %>%
#     filter(pageName %in% wikiCompanyList$wikiPageName) %>%
#     group_by(pageName) %>%
#     summarise(viewNums = sum(as.numeric(viewNums), na.rm = T)) %>%
#     transmute(dataDate, dataTime, pageName, viewNums)
#
#   # 儲存資料
#   storeTable <- storeTable %>% bind_rows(keyPage)
# }
#
# # 刪除資料
# file.remove(gsub(".gz", "", targetFilePathName))
}
for(ix in c(7:nrow(targetYearMonth))){
# 目標頁面網址
url <- paste0("https://dumps.wikimedia.org/other/pagecounts-raw/",
targetYearMonth$year[ix],"/", targetYearMonth$yearMonth[ix],"/")
dir.create(targetYearMonth$yearMonth[ix])
# 建立儲存表
# storeTable <- NULL
# 取得頁面連結資訊
downloadFileName <- read_html(url) %>%
html_nodes(xpath = "//li/a") %>%
html_text()
# 只下載pagecounts頁面(在2010年5月時會有pagecounts和projectcount連結)
downloadFileName <- downloadFileName[grepl("pagecounts", downloadFileName)]
# 迴圈頁面連結資訊以下載資料
filePath <-paste0("D:/MiaoLing Project/wiki",'/',targetYearMonth$yearMonth[ix])
iy <- 1
for(iy in c(1:length(downloadFileName))){
cat(paste0("目前正在下載年月: ", targetYearMonth$yearMonth[ix]), "進度: ", iy, "/", length(downloadFileName),"\n")
# 防呆機制:(20181210:發現部分檔案有毀損狀況導致無法順利解壓縮 故做此處理)
if(downloadFileName[iy] %in% c("pagecounts-20130618-150000.gz")){
next
}
# 資料名稱
targetFilePathName <- paste0(filePath, "/", downloadFileName[iy])
# 切割名稱方便儲存資訊
dataDate <- strsplit(downloadFileName[iy], split = "-")[[1]][2] %>% as.numeric()
dataTime <- strsplit(downloadFileName[iy], split = "-")[[1]][3]
dataTime <- strsplit(dataTime, split = "\\.")[[1]][1] %>% as.numeric()
dataTime <- floor(dataTime/10000)
# 下載資料
download.file(url = paste0(url, downloadFileName[iy]), destfile = targetFilePathName)
# 解壓縮
#gunzip(filename = targetFilePathName, overwrite = T)
## 判斷是否為空資料 若為空資料則不做併表 直接刪除資料進入下一階段
# if(file.size(gsub(".gz", "", targetFilePathName)) != 0){
#
#
#   # 讀取資料
#   # 此處略過前5行不讀取 因為前5行有時候是亂碼 R看不懂
#   # 略過5行沒關係 因為重點是要抓language=en的資料 language是按字母順序排的
#   data <- read.table(file = gsub(".gz", "", targetFilePathName), quote="", sep = " " ,
#                      header = F, stringsAsFactors= F, fill = TRUE, skip = 2000) %>%
#     as_data_frame() %>%
#     rename(language = V1, pageName = V2, viewNums = V3)
#
#   # 判斷是否有要找的維基關鍵頁面
#   keyPage <- data %>%
#     filter(language == "en") %>%
#     filter(pageName %in% wikiCompanyList$wikiPageName) %>%
#     group_by(pageName) %>%
#     summarise(viewNums = sum(as.numeric(viewNums), na.rm = T)) %>%
#     transmute(dataDate, dataTime, pageName, viewNums)
#
#   # 儲存資料
#   storeTable <- storeTable %>% bind_rows(keyPage)
# }
#
# # 刪除資料
# file.remove(gsub(".gz", "", targetFilePathName))
}
# 由每小時頻率轉為每日資料
# storeTable <- storeTable %>%
#   group_by(dataDate, pageName) %>%
#   summarise(viewNums = sum(viewNums, na.rm = T))
#
# # 本月份資料跑完 儲存資料
# write.csv(storeTable, file = paste0(targetYearMonth$yearMonth[ix], "-wiki-page-view-numbers.csv"), row.names = F)
}
targetYearMonth$yearMonth[ix]
ix = 6
targetYearMonth$yearMonth[ix]
# 迴圈頁面連結資訊以下載資料
filePath <-paste0("D:/MiaoLing Project/wiki",'/',targetYearMonth$yearMonth[ix])
# 目標頁面網址
url <- paste0("https://dumps.wikimedia.org/other/pagecounts-raw/",
targetYearMonth$year[ix],"/", targetYearMonth$yearMonth[ix],"/")
# 取得頁面連結資訊
downloadFileName <- read_html(url) %>%
html_nodes(xpath = "//li/a") %>%
html_text()
# 只下載pagecounts頁面(在2010年5月時會有pagecounts和projectcount連結)
downloadFileName <- downloadFileName[grepl("pagecounts", downloadFileName)]
for(iy in c(628:length(downloadFileName))){
cat(paste0("目前正在下載年月: ", targetYearMonth$yearMonth[ix]), "進度: ", iy, "/", length(downloadFileName),"\n")
# 防呆機制:(20181210:發現部分檔案有毀損狀況導致無法順利解壓縮 故做此處理)
if(downloadFileName[iy] %in% c("pagecounts-20130618-150000.gz")){
next
}
# 資料名稱
targetFilePathName <- paste0(filePath, "/", downloadFileName[iy])
# 切割名稱方便儲存資訊
dataDate <- strsplit(downloadFileName[iy], split = "-")[[1]][2] %>% as.numeric()
dataTime <- strsplit(downloadFileName[iy], split = "-")[[1]][3]
dataTime <- strsplit(dataTime, split = "\\.")[[1]][1] %>% as.numeric()
dataTime <- floor(dataTime/10000)
# 下載資料
download.file(url = paste0(url, downloadFileName[iy]), destfile = targetFilePathName)
# 解壓縮
#gunzip(filename = targetFilePathName, overwrite = T)
## 判斷是否為空資料 若為空資料則不做併表 直接刪除資料進入下一階段
# if(file.size(gsub(".gz", "", targetFilePathName)) != 0){
#
#
#   # 讀取資料
#   # 此處略過前5行不讀取 因為前5行有時候是亂碼 R看不懂
#   # 略過5行沒關係 因為重點是要抓language=en的資料 language是按字母順序排的
#   data <- read.table(file = gsub(".gz", "", targetFilePathName), quote="", sep = " " ,
#                      header = F, stringsAsFactors= F, fill = TRUE, skip = 2000) %>%
#     as_data_frame() %>%
#     rename(language = V1, pageName = V2, viewNums = V3)
#
#   # 判斷是否有要找的維基關鍵頁面
#   keyPage <- data %>%
#     filter(language == "en") %>%
#     filter(pageName %in% wikiCompanyList$wikiPageName) %>%
#     group_by(pageName) %>%
#     summarise(viewNums = sum(as.numeric(viewNums), na.rm = T)) %>%
#     transmute(dataDate, dataTime, pageName, viewNums)
#
#   # 儲存資料
#   storeTable <- storeTable %>% bind_rows(keyPage)
# }
#
# # 刪除資料
# file.remove(gsub(".gz", "", targetFilePathName))
}
setwd("~/")
install.packages("dm")
rm(list= ls());gc()
library(dm)
cdm_nycflights13()
install.packages("cdm_nycflights13")
library(cdm_nycflights13)
install.packages("CDM")
?cdm
??cdm_add_fk
library(CDM)
cdm_nycflights13()
cdm
install.packages("nycflights13")
library(nycflights13)
nycflights13()
flights
a = flights
str(a)
rm(list= ls());gc()
library(dm)
library(nycflights13)
f = flights
a = airports
p = planes
cdm_nycflights13()
cdm_nycflights13(cycle = FALSE) %>%
cdm_join_to_tbl(airports, flights, join = semi_join)
cdm_nycflights13(cycle = FALSE) %>%
dm_join_to_tbl(airports, flights, join = semi_join)
c = cdm_nycflights13()
d = cdm_nycflights13(cycle = FALSE) %>%
dm_join_to_tbl(airports, flights, join = semi_join)
d = cdm_nycflights13(cycle = FALSE) %>%
cdm_join_to_tbl(airports, flights, join = semi_join)
c = cdm_nycflights13(cycle = FALSE)
rm(list= ls());gc()
library(dm)
library(nycflights13)
f = flights
a = airports
p = planes
c = cdm_nycflights13(cycle = FALSE)
d = cdm_nycflights13(cycle = FALSE) %>%
cdm_join_to_tbl(airports, flights, join = semi_join)
pnorm(0.9)
rnorm(0.9)
qnorm(0.9)
qnorm(3)
?qnorm
qnorm(0.954)
qnorm(0.977)
0.05/12
100*(1.004)**(28*12)
rm(list = ls())
gc()
data = data.frame(period = seq(1,28*12))
View(data)
data = data %>% mutate(FV = 100*(1.004)**period )
library(tidyverse)
data = data %>% mutate(FV = 100*(1.004)**period )
View(data)
sum(data$FV)
rm(list= ls());gc()
library(data.table)
library(tidyverse)
library(updateR)
library(installr)
updateR()
updateR()
rm(list= ls());gc()
library(data.table)
library(tidyverse)
#----Sales Develpoment---#
file_name = 'C:\Users\USER\Desktop\ESSCA Course\Sales Development\ATOUR TAGHIPOUR\INT511_Data_Session1.xlsx'
#----Sales Develpoment---#
file_name = 'C:\\Users\\USER\\Desktop\\ESSCA Course\\Sales Development\\ATOUR TAGHIPOUR\\INT511_Data_Session1.xlsx'
df = fread(file_name,data.table = F)
#----Sales Develpoment---#
file_name = 'C:\Users\USER\Desktop\ESSCA Course\Sales Development\ATOUR TAGHIPOUR\INT511_Data_Session1.xlsx'
library(xlsx)
#----Sales Develpoment---#
file_name = 'C:/Users/USER/Desktop/ESSCA Course/Sales Development/ATOUR TAGHIPOUR/INT511_Data_Session1.xlsx'
df = readxl::read_xlsx(file_name,data.table = F)
df = readxl::read_xlsx(file_name)
View(df)
df = readxl::read_xlsx(file_name,skip = 2)
View(df)
library(readxl)
MAD = mean(df$WebVisits,na.rm = T)
df = readxl::read_xlsx(file_name,skip = 2) %>% mutate(F_visits = lag(WebVisits))
MAD = mean(abs(df$WebVisits-df$F_visits,na.rm = T)
MAD = mean(abs(df$WebVisits-df$F_visits),na.rm = T)
MSE = sum((df$WebVisits-df$F_visits)**2)
(df$WebVisits-df$F_visits)
MSE = sum((df$WebVisits-df$F_visits)**2,na.rm = T)
(df$WebVisits-df$F_visits)**2
length(df)
nrow(df %>% filter(!is.na(F_visits)))
MSE = sum((df$WebVisits-df$F_visits)**2,na.rm = T)/nrow(df %>% filter(!is.na(F_visits)))
MSE = sum((df$WebVisits-df$F_visits)**2,na.rm = T)/(nrow(df %>% filter(!is.na(F_visits)))-1)
MAPE = mean(abs(df$WebVisits-df$F_visits)/df$WebVisits,na.rm = T)*100
library(stats)
library(TTR)
MovingAvg = SMA(df$WebVisits,3)
MovingAvg = lag(SMA(df$WebVisits,3))
MovingAvg = lag(SMA(df$WebVisits,period))
#Forecast
period = 3
MovingAvg = lag(SMA(df$WebVisits,period))
?weighted.mean
WeightAvg = weighted.mean(df$WebVisits, w=c(0.1,0.2,0.3,0.4))
install.packages("pracma")
library(pracma)
?movavg
WeightAvg = movavg(df$WebVisits, w=c(0.1,0.2,0.3,0.4))
?rollify
library(tibbletime)
#--functiom--#
roll_WA = rollify(weighted.mean,4)
#--functiom--#
roll_WA = rollify(weighted.mean( w=c(0.1,0.2,0.3,0.4)),4)
#--functiom--#
roll_WA = rollify(weighted.mean(x, w=c(0.1,0.2,0.3,0.4)),4)
#--functiom--#
roll_WA = rollify(weighted.mean,4)
WeightAvg = roll_WA(df$WebVisits, w=c(0.1,0.2,0.3,0.4))
df$WebVisits
#--functiom--#
roll_WA = rollify(weighted.mean,window = 4)
WeightAvg = roll_WA(df$WebVisits, w=c(0.1,0.2,0.3,0.4))
w=c(0.1,0.2,0.3,0.4)
w[4]
WeightAvg = lag(df$WebVisits)*w[4]+lag(df$WebVisits,2)*w[3]+
lag(df$WebVisits,3)*w[2]+lag(df$WebVisits,4)*w[1]
lag(df$WebVisits)
learning_rate = 0.1
EMA = learning_rate*lag(df$WebVisits)+(1-learning_rate)*lag(df$F_visits)
#Trend_analysis
TPA = (df$WebVisits-lag(df$WebVisits))/lag(df$WebVisits)
#Trend_analysis
TPA = mean((df$WebVisits-lag(df$WebVisits))/lag(df$WebVisits),na.rm = T)
TPA_Forcast = (1+TPA)*last(df$WebVisits)
#Regression
regressor = lm(data= df, formula = WebVisits~Month)
summary(regressor)
a = summary(regressor)
a$df
a$coefficients
library(ggplot)
library(ggplot2)
coeff = a$coefficients
ggplot(data = df, aes(x = Month, y = WebVisits))+geom_point(col = 'blue')
ggplot(data = df, aes(x = Month, y = WebVisits))+
geom_point(col = 'blue')+
theme_light()
ggplot(data = df, aes(x = Month, y = WebVisits))+
geom_point(col = 'blue')+
geom_abline(intercept = coeff[1,1],slope = coeff[2,1])
ggplot(data = df, aes(x = Month, y = WebVisits))+
geom_point(col = 'blue')+
geom_abline(intercept = coeff[1,1],slope = coeff[2,1],col= 'red')
theme_light()+geom_tile('Regression Result')
theme_light()+geom_title('Regression Result')
theme_light()+labs(title = 'Regression Result')
theme_light()+ggtitle('Regression Result')
theme_light()+ggtitle('Regression Result')
ggplot(data = df, aes(x = Month, y = WebVisits))+
geom_point(col = 'blue')+
geom_abline(intercept = coeff[1,1],slope = coeff[2,1],col= 'red')+
theme_light()+ggtitle('Regression Result')
ggplot(data = df, aes(x = Month, y = WebVisits))+
geom_point(col = 'blue')+
geom_abline(intercept = coeff[1,1],slope = coeff[2,1],col= 'red')+
theme_classic()+ggtitle('Regression Result')
#seasonality adjustment
season_factor = c(1.2,1.3,1.3,1.1,0.8,0.7,0.8,0.6,0.7,1,1.1,1.4)
season_factor
#seasonality adjustment
season_factor = c(1.2, 1.3, 1.3, 1.1, 0.8, 0.7, 0.8, 0.6, 0.7, 1.0, 1.1, 1.4)
fitted(regressor)
fitted(regressor)
#predict
predict(regressor, data.frame(Month = c(15,16)))
#predict
predict(regressor, data.frame(Month = c(15,16)))*season_factor[c(3,4)]
2833.390*1.3
3032.093*1.1
pnorm(0.975)
qnorm(0.975)
pnorm(2)
qnorm(2)
pnorm(2)
(1-pnorm(2))*2
qnorm(0.977)
qnorm(0.92)
sqrt(5)
75/2*3000+6000*25
75*3000
qnorm(0.5)
3000+sqrt(60000+250000*4)
sqrt(2*300*20/1.5)
sqrt(2*300*20/1.2)
sqrt(2*300*20/18)
sqrt(2*50*8000/20)
1.036*sqrt(60000+250000*4)
Ss = qnorm(0.95)*sqrt(2*((100)**2)+((25000/52)**2)*(0**2))
3000+qnorm(0.9)*sqrt(60000+250000*4)
qnorm(0.9)
50/sqrt(200)
sqrt(2*2500*8/4)
sqrt(2*50*1000/(0.25*10))
sqrt(2*50*1000/(0.25*9.8))
sqrt(2*50*1000/(0.25*9.6))
P = 10
K = 50
D = 1000
h = 0.25*P
EOQ = sqrt(2*K*D/h)
P = 10
K = 50
D = 1000
h = 0.25*P
EOQ = sqrt(2*K*D/h)
TC = K*D/EOQ+h*Q/2+P*D
TC = K*D/EOQ+h*EOQ/2+P*D
K*D/EOQ
h*EOQ/2
P = 9.8
EOQ = 202
TC = K*D/EOQ+h*EOQ/2+P*D
P = 9.6
EOQ = 500
TC = K*D/EOQ+h*EOQ/2+P*D
sqrt(60000+250000*4)
setwd("C:/Users/USER/Desktop/FR_learning/simpleFR_tk")
rm(list = ls());gc()
library(data.table)
library(tidyverse)
df = fread('FR_vocab.csv', data.table = F)
View(df)
df %>% unite("z", V1:V5, remove = FALSE)
A = df %>% unite("z", V1:V5, remove = FALSE)
View(A)
df = fread('FR_vocab.csv', data.table = F, encoding = 'utf-8')
rm(list = ls());gc()
library(data.table)
library(tidyverse)
df = fread('FR_vocab.csv', data.table = F, encoding = 'utf-8')
df = fread('FR_vocab.csv', data.table = F, encoding = 'UTF-8')
View(df)
View(df)
View(df)
df = fread('FR_vocab.csv', data.table = F, encoding = 'Latin-1')
View(df)
